{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "# 신경망 클래스의 정의\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # 신경망 초기화하기\n",
    "    def __init__(self, _inputNodes, _hiddenNodes, _outputNodes, _learningRate):\n",
    "        '''\n",
    "            하나의 클래스에 신경망의 크기를 전달함으로써 신경망 생성이 가능하며,\n",
    "            마찬가지로 학습률(learning rate)도 전달이 가능하다.\n",
    "        '''\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "        self.i_nodes = _inputNodes\n",
    "        self.h_nodes = _hiddenNodes\n",
    "        self.o_nodes = _outputNodes\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "        # 신경망에서 가장 중요한 부분이 바로 연결 노드의 가중치이다.\n",
    "        # 가중치는 전파시 전달되는 신호와 역전파 시 오차를 계산하는 데 쓰이며, \n",
    "        # 이를 통해 신경망을 개선하는 역할을 수행한다.\n",
    "\n",
    "        # 가중치는 행렬로 간결하게 표현될 수 있다.\n",
    "        # 각각의 가중치는 임의의 작은 값으로 초기화 한다.\n",
    "\n",
    "        # 1. (은닉 노드 x 입력 노드)의 크기를 갖는 입력 계층과 은닉 계층 사이의 가중치의 행렬\n",
    "        # W(input_hidden)\n",
    "        # 2. (출력노드 x 은닉 노드)의 크기를 갖는 은닉 계층과 출력 계층 사이의 가중치의 행렬\n",
    "        # W(hidden_output)\n",
    "\n",
    "        # numpy를 이용해서 0과 1 사이의 임의로 선택한 값을 원소로 가지는 행렬을 생성.\n",
    "        # 이 행렬의 크기는 (행 x 열)\n",
    "\n",
    "        # input -> hidden\n",
    "        # self.wih = (np.random.rand(self.h_nodes, self.i_nodes) - 0.5)\n",
    "\n",
    "        # hidden -> output\n",
    "        # self.who = (np.random.rand(self.o_nodes, self.h_nodes) - 0.5)\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "        # 가중치를 임의의 값으로 초기화하는 더 정교한 방법이 있음.\n",
    "        # 이 경우, 가중치는 0을 중심으로 하여 1/sqrt(들어오는 연결 노드의 개수)의 표준 편차를 갖는\n",
    "        # 정규분포에 따라 구하도록 한다.\n",
    "        # numpy의 numpy.random.normal() 함수를 활용하면 이를 쉽게 구현할 수 있다.\n",
    "        # 이 변수에 필요한 매개변수는 정규분포의 중심, 표준편차, numpy 행렬이다.\n",
    "        \n",
    "        # input -> hidden\n",
    "        # Python에서 노드로 들어오는 연결 노드의 개수에 루트를 씌우고 역수를 취한 표준편차는 파이썬의 문법으로 아래와 같이 표현됨.\n",
    "        # 1. 정규분포의 중심은 0.0\n",
    "        # 2. 표준편차\n",
    "        # 3. 우리가 원하는 numpy 행렬\n",
    "        self.wih = np.random.normal(0.0, pow(self.i_nodes, -0.5), (self.h_nodes, self.i_nodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.h_nodes, -0.5), (self.o_nodes, self.h_nodes))\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "        # 학습률(learning rate)\n",
    "        self.lr = _learningRate\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # 활성화 함수\n",
    "        # 활성화 함수에 약간의 변화를 주거나, 아니면 때로는 다른 활성화 함수로 교체하는 경우가 있을 수 있으므로,\n",
    "        # 활성화 함수를 신경망 객체의 초기화 부분에 정의해 두는 것이 좋다.\n",
    "        \n",
    "        # lambda 함수로 전달받은 인자 x에 대하여 scipy.special.expit(시그모이드 함수)에 x를 넣은 값을 return한다.\n",
    "        # 일종의 anonymous function이며, activation_function에 할당된 것이다.\n",
    "        self.activation_function = lambda x: scipy.special.expit(x) \n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "    # 신경망 학습시키기\n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    # 신경망에 질의하기\n",
    "    def query(self, _inputs_list):\n",
    "        '''\n",
    "            신경망으로 들어오는 입력을 받아 출력을 반환한다.\n",
    "            입력 계층부터 은닉 계층을 거쳐 최종 출력 계층까지 수행한다.\n",
    "            또한, 신호는 은닉 노드와 출력 노드로 전달될 때 가중치 연산과 활성화 함수 적용을 거친다.\n",
    "        '''\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "        \n",
    "        # 입력 리스트를 2차원 행렬로 변환\n",
    "        \n",
    "        # 우선 1차원 array로 데이터를 넘기면, np.array를 통해서\n",
    "        # 2차원 matrix로 변환하는 것으로 생각하면 될듯.\n",
    "        inputs = np.array(_inputs_list, ndmin=2).T # 뒤에 붙은 T는 transpose?\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # 은닉 계층으로 들어오는 신호를 계산\n",
    "        \n",
    "        # numpy의 dot 함수를 통해서 은닉 계층의 각 노드로 들어오는 신호를 계산한다.\n",
    "        # 내적을 이용하면 입력 계층이나 은닉 계층의 노드 수가 달라지더라도 코드를 다시 작성할 필요가 없다.\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # 은닉 계층에서 나가는 신호를 계산\n",
    "        \n",
    "        # 이제 은닉계층으로부터 나오는 신호를 구하려면, 시그모이드 함수를 적용하는 일만 하면 된다.\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        # activation_function을 거치면, 은닉 계층에서 나가는 신호들은 hidden_outputs에 저장된다.\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # 최종 출력 계층으로 들어오는 신호를 계산\n",
    "        \n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        # 최종 출력 계층에서 나가는 신호를 계산\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        return final_outputs\n",
    "\n",
    "        # 이후에 노드의 개수(hidden_layer)가 많아지면 for loop을 돌려서 학습시키는 것인듯?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adda71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력, 은닉, 출력 노드의 수\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# 학습률은 0.3으로 정의\n",
    "learning_rate = 0.3\n",
    "\n",
    "# 신경망의 인스턴스를 생성\n",
    "n = NeuralNetwork(inputNodes=input_nodes, hiddenNodes=hidden_nodes, outputNodes=output_nodes, learningRate=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
